# QQP Training Configuration
# Extends default.yaml with QQP-specific settings

dry_run: false
use_exca: false


# Data paths
training_data_path: "datasets/tokenized/bert-base-uncased/QQP-Official/train"
validation_data_path: "datasets/tokenized/bert-base-uncased/QQP-Official/valid"
padding_strategy:
  mark_first_padding: true
  mark_second_padding: true

check_val_every_n_epoch: 10
val_interval: null

denoising_step_size: 512  # Step size for denoising process
num_val_batches_to_log: 1  # Number of validation batches to log predictions for
num_timestep_bins: 4
prediction_shortcut_size: 512
log_train_predictions_every_n_epochs: 20  # Number of epochs between train prediction logging
log_train_predictions_from_n_epochs: 1  # Number of epochs between train prediction logging

limit_train_batches: 2
limit_val_batches: 2
overfit_batches: 2

max_steps: 200
gradient_clipping: null
reduce_fn: "mean"

batch_size: 1024
accumulate_grad_batches: 2

architecture: "transformer"

seed: 44

normalize_flow_matching_loss: false

flow_matching_loss_weight: 1.0
consistency_loss_weight: 1.0
nll_loss_weight: 1.0
isotropy_loss_weight: 0.0

self_consistency_ratio: 0.0
model:
  init_pretrained: bert
  config_name: bert-base-uncased
  use_pretrained_weights: false
  freeze_word_embedding: false
  vocab_size: 30522
  word_embedding_std: 1.0
  input_dims: 128
  output_dims: 128
  sc_rate: 0.5
  parametrization: "x0"
  stacked_embeddings: false
  hidden_t_dim: 128
  hidden_shortcut_dim: null
  projection_activation: "tanh"
  normalize_word_embedding: false
  scale_time: false

# Optimizer configuration
optimizer:
  scheduler:
    lr: 1e-4
    type: "linear"
    start_factor: 1.0
    end_factor: 0.0
    total_steps: 2000

#ema: null

# WandB configuration
wandb:
  project_name: "FMSeqtest"
#  run_name: "custom dataset"
#  project_name: "test_v2"
#  project_name: "test"
  enabled: true

# Checkpoint configuration
checkpoint:
  save_folder: "checkpoints/qqp"